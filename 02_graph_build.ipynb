{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5914763a",
   "metadata": {},
   "source": [
    "# Day 2: Graph Building & Core Metrics\n",
    "\n",
    "**Objective**: Build multilayer social graph from CNS data and implement core tie-strength metrics.\n",
    "\n",
    "**Deliverables**:\n",
    "- `build_multilayer_graph(df)` → `nx.MultiDiGraph`\n",
    "- `collapse_edges(G)` → weighted `nx.DiGraph`\n",
    "- 10 metric callables (≤ 25 lines each)\n",
    "- Export `graph.pkl`, `node_metrics.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed5171",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2358c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "02 – Build Social Graph & Metrics (CNS POC)\n",
    "------------------------------------------------\n",
    "Reads all Parquet partitions, constructs a multilayer graph, collapses it\n",
    "into a single weighted digraph, and exposes the nine analytics callables.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d5849f",
   "metadata": {
    "title": "[imports]"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from datetime import timedelta\n",
    "from utils.device import get_device  # GPU helper (currently unused but kept for future GNN work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979a338",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[constants]"
   },
   "outputs": [],
   "source": [
    "SMS_WEIGHT: int = 30          # seconds‑credit per SMS\n",
    "TREND_WIN_DAYS: int = 7       # rolling‑window size for trend/churn\n",
    "MIN_EVENTS_PREF: int = 5      # min interactions to label channel preference\n",
    "PREF_THRESHOLD: float = 0.60  # 60 % rule\n",
    "MIN_WEIGHT_TOPTIE: int = 90   # ignore ties < 90 s total weight\n",
    "CHURN_DROP_RATIO: float = 0.40  # current wk inbound < 40 % prev wk\n",
    "\n",
    "RAW_PARQUET_DIR = Path(\"parquet\")\n",
    "GRAPH_PKL = Path(\"graph.pkl\")\n",
    "NODE_CSV = Path(\"node_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1ddab",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[loader]"
   },
   "outputs": [],
   "source": [
    "def load_all_partitions(parquet_dir: Path = RAW_PARQUET_DIR) -> pd.DataFrame:\n",
    "    \"\"\"Load every Parquet file under *parquet_dir* into one DataFrame.\"\"\"\n",
    "    parts = list(parquet_dir.rglob(\"*.parquet\"))\n",
    "    if not parts:\n",
    "        raise FileNotFoundError(\"No Parquet files found – run 01_ingest first.\")\n",
    "    return pd.concat((pd.read_parquet(p) for p in parts), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e2761",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[graph builders]"
   },
   "outputs": [],
   "source": [
    "def build_multilayer_graph(df: pd.DataFrame) -> nx.MultiDiGraph:\n",
    "    \"\"\"Return MultiDiGraph with separate edges for call vs sms (directed).\"\"\"\n",
    "    G = nx.MultiDiGraph()\n",
    "    for row in df.itertuples(index=False):\n",
    "        weight = row.duration if row.channel == \"call\" else SMS_WEIGHT\n",
    "        key = row.channel  # distinguishes parallel edges\n",
    "        G.add_edge(row.src, row.dst, key=key, channel=row.channel, weight=weight)\n",
    "    return G\n",
    "\n",
    "\n",
    "def collapse_edges(mg: nx.MultiDiGraph) -> nx.DiGraph:\n",
    "    \"\"\"Sum weights per direction; keep channel‑specific counts in attributes.\"\"\"\n",
    "    cg = nx.DiGraph()\n",
    "    for u, v, data in mg.edges(data=True):\n",
    "        # Get existing edge data or create new\n",
    "        if cg.has_edge(u, v):\n",
    "            edge_data = cg[u][v]\n",
    "        else:\n",
    "            edge_data = {\n",
    "                \"weight\": 0,\n",
    "                \"call_weight\": 0,\n",
    "                \"sms_weight\": 0,\n",
    "                \"call_count\": 0,\n",
    "                \"sms_count\": 0,\n",
    "            }\n",
    "            cg.add_edge(u, v, **edge_data)\n",
    "        \n",
    "        if data[\"channel\"] == \"call\":\n",
    "            edge_data[\"call_weight\"] += data[\"weight\"]\n",
    "            edge_data[\"call_count\"] += 1\n",
    "        else:\n",
    "            edge_data[\"sms_weight\"] += data[\"weight\"]\n",
    "            edge_data[\"sms_count\"] += 1\n",
    "        edge_data[\"weight\"] = edge_data[\"call_weight\"] + edge_data[\"sms_weight\"]\n",
    "    return cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34e802",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[callables]"
   },
   "outputs": [],
   "source": [
    "\n",
    "def top_ties(G: nx.DiGraph, user: int, k: int = 5, min_weight: int = MIN_WEIGHT_TOPTIE) -> List[Tuple[int, float]]:\n",
    "    neigh: Dict[int, float] = {}\n",
    "    for nbr in G.successors(user):\n",
    "        neigh[nbr] = neigh.get(nbr, 0) + G[user][nbr][\"weight\"]\n",
    "    for nbr in G.predecessors(user):\n",
    "        neigh[nbr] = neigh.get(nbr, 0) + G[nbr][user][\"weight\"]\n",
    "    return sorted(\n",
    "        [(n, w) for n, w in neigh.items() if w >= min_weight],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True,\n",
    "    )[:k]\n",
    "\n",
    "\n",
    "def channel_preference(G: nx.DiGraph, a: int, b: int) -> str:\n",
    "    edge_ab = G.get_edge_data(a, b, default={})\n",
    "    edge_ba = G.get_edge_data(b, a, default={})\n",
    "    sms = edge_ab.get(\"sms_count\", 0) + edge_ba.get(\"sms_count\", 0)\n",
    "    calls = edge_ab.get(\"call_count\", 0) + edge_ba.get(\"call_count\", 0)\n",
    "    total = sms + calls\n",
    "    if total < MIN_EVENTS_PREF:\n",
    "        return \"undetermined\"\n",
    "    sms_ratio = sms / total\n",
    "    if sms_ratio >= PREF_THRESHOLD:\n",
    "        return \"sms\"\n",
    "    if sms_ratio <= 1 - PREF_THRESHOLD:\n",
    "        return \"call\"\n",
    "    return \"balanced\"\n",
    "\n",
    "\n",
    "def reciprocity(G: nx.DiGraph, user: int) -> float:\n",
    "    in_w = sum(data[\"weight\"] for _, _, data in G.in_edges(user, data=True))\n",
    "    total_w = in_w + sum(data[\"weight\"] for _, _, data in G.out_edges(user, data=True))\n",
    "    return 0.0 if total_w == 0 else in_w / total_w\n",
    "\n",
    "\n",
    "def detect_circles(G: nx.DiGraph, user: int) -> List[int]:\n",
    "    import networkx.algorithms.community as nx_comm\n",
    "\n",
    "    UG = nx.Graph()\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        UG.add_edge(u, v, weight=data[\"weight\"])\n",
    "    for res in [1.0, 0.5, 1.5]:  # retry with diff resolution if degenerate\n",
    "        part = nx_comm.louvain_communities(UG, weight=\"weight\", resolution=res)\n",
    "        if 1 < len(part) < len(UG):\n",
    "            break\n",
    "    for cid, community in enumerate(part):\n",
    "        if user in community:\n",
    "            return list(community)\n",
    "    return []\n",
    "\n",
    "\n",
    "def relationship_trend(df_pair: pd.DataFrame) -> str:\n",
    "    if df_pair.empty:\n",
    "        return \"insufficient_data\"\n",
    "    df_pair = df_pair.set_index(\"sent\").sort_index()\n",
    "    weekly = df_pair[\"weight\"].resample(f\"{TREND_WIN_DAYS}D\").sum()\n",
    "    if len(weekly) < 3:\n",
    "        return \"insufficient_data\"\n",
    "    change = (weekly.iloc[-1] - weekly.iloc[-2]) / max(1e-6, weekly.iloc[-2])\n",
    "    if change > 0.25:\n",
    "        return \"growing\"\n",
    "    if change < -0.25:\n",
    "        return \"fading\"\n",
    "    return \"stable\"\n",
    "\n",
    "\n",
    "def avg_reply_delay(df_pair: pd.DataFrame) -> float | None:\n",
    "    if df_pair.empty:\n",
    "        return None\n",
    "    df_pair = df_pair.sort_values(\"sent\")\n",
    "    delays = []\n",
    "    last_out_ts = None\n",
    "    last_out_sender = None\n",
    "    for row in df_pair.itertuples(index=False):\n",
    "        s, d, ts = row.src, row.dst, row.sent\n",
    "        if last_out_ts is not None and last_out_sender != s:\n",
    "            delays.append((ts - last_out_ts).total_seconds())\n",
    "        last_out_ts, last_out_sender = ts, s\n",
    "    return None if not delays else sum(delays) / len(delays)\n",
    "\n",
    "\n",
    "def extrovert_score(G: nx.DiGraph, top_n: int = 10) -> List[Tuple[int, float]]:\n",
    "    deg = {n: sum(data[\"weight\"] for _, _, data in G.out_edges(n, data=True)) for n in G}\n",
    "    bet = nx.betweenness_centrality(G, weight=\"weight\", normalized=True)\n",
    "    score = {n: deg.get(n, 0) + bet.get(n, 0) * 1000 for n in G}\n",
    "    return sorted(score.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "\n",
    "def churn_drop(G: nx.DiGraph, user: int) -> Tuple[bool, float]:\n",
    "    inbound = [data[\"weight\"] for _, _, data in G.in_edges(user, data=True)]\n",
    "    if not inbound:\n",
    "        return False, 0.0\n",
    "    # Build per‑edge DataFrame of timestamps & weights\n",
    "    rows = []\n",
    "    for src, _, data in G.in_edges(user, data=True):\n",
    "        rows.extend([(src, t, w) for t, w in data.get(\"timestamps\", [])])\n",
    "    if not rows:\n",
    "        return False, 0.0\n",
    "    df = pd.DataFrame(rows, columns=[\"src\", \"sent\", \"weight\"]).set_index(\"sent\").sort_index()\n",
    "    weekly = df[\"weight\"].resample(\"7D\").sum()\n",
    "    if len(weekly) < 2:\n",
    "        return False, 0.0\n",
    "    ratio = weekly.iloc[-1] / max(1e-6, weekly.iloc[-2])\n",
    "    return ratio < CHURN_DROP_RATIO, ratio\n",
    "\n",
    "\n",
    "def find_spam_nodes(G: nx.DiGraph) -> List[int]:\n",
    "    suspects = []\n",
    "    for n in G.nodes:\n",
    "        out_w = sum(data[\"weight\"] for _, _, data in G.out_edges(n, data=True))\n",
    "        in_w = sum(data[\"weight\"] for _, _, data in G.in_edges(n, data=True))\n",
    "        if out_w >= 20 * SMS_WEIGHT and (in_w == 0 or out_w / max(1, in_w) > 10):\n",
    "            if reciprocity(G, n) < 0.1:\n",
    "                suspects.append(n)\n",
    "    return suspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c199c33",
   "metadata": {
    "title": "[driver]"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_full = load_all_partitions()\n",
    "\n",
    "    # Build graphs\n",
    "    mg = build_multilayer_graph(df_full)\n",
    "    cg = collapse_edges(mg)\n",
    "\n",
    "    # Save graph for Day‑3 use\n",
    "    with GRAPH_PKL.open(\"wb\") as f:\n",
    "        pickle.dump(cg, f)\n",
    "\n",
    "    # Produce node‑level metrics CSV\n",
    "    rows = []\n",
    "    for n in cg.nodes:\n",
    "        rows.append({\n",
    "            \"node\": n,\n",
    "            \"reciprocity\": reciprocity(cg, n),\n",
    "            \"out_weight\": sum(d[\"weight\"] for _, _, d in cg.out_edges(n, data=True)),\n",
    "            \"in_weight\": sum(d[\"weight\"] for _, _, d in cg.in_edges(n, data=True)),\n",
    "        })\n",
    "    pd.DataFrame(rows).to_csv(NODE_CSV, index=False)\n",
    "\n",
    "    print(\"Graph construction complete →\", GRAPH_PKL)\n",
    "    print(\"Node metrics written →\", NODE_CSV)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
