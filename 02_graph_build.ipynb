{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5914763a",
   "metadata": {},
   "source": [
    "# Day 2: Graph Building & Core Metrics\n",
    "\n",
    "**Objective**: Build multilayer social graph from CNS data and implement core tie-strength metrics.\n",
    "\n",
    "**Deliverables**:\n",
    "- `build_multilayer_graph(df)` → `nx.MultiDiGraph`\n",
    "- `collapse_edges(G)` → weighted `nx.DiGraph`\n",
    "- 10 metric callables (≤ 25 lines each)\n",
    "- Export `graph.pkl`, `node_metrics.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pickle\n",
    "from utils.device import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d5849f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load the processed data\n",
    "print(\"Loading processed CNS data...\")\n",
    "df = pd.read_parquet(\"parquet/year_month=2013-09.parquet\")\n",
    "print(f\"Loaded {len(df)} interactions\")\n",
    "print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Unique users: {df['src'].nunique()}\")\n",
    "print(f\"Channels: {df['channel'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979a338",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_multilayer_graph(df: pd.DataFrame) -> nx.MultiDiGraph:\n",
    "    \"\"\"\n",
    "    Build multilayer directed graph from CNS interaction data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['src', 'dst', 'timestamp', 'duration', 'channel']\n",
    "    \n",
    "    Returns:\n",
    "        MultiDiGraph with edge attributes: layer, duration, sent\n",
    "    \"\"\"\n",
    "    G = nx.MultiDiGraph()\n",
    "    \n",
    "    # Add nodes (all unique users)\n",
    "    all_users = set(df['src'].unique()) | set(df['dst'].unique())\n",
    "    G.add_nodes_from(all_users)\n",
    "    \n",
    "    # Add edges with layer information\n",
    "    for _, row in df.iterrows():\n",
    "        src, dst = row['src'], row['dst']\n",
    "        duration = row.get('duration', 0)  # SMS has no duration\n",
    "        channel = row['channel']\n",
    "        \n",
    "        # Edge attributes\n",
    "        edge_attrs = {\n",
    "            'layer': channel,\n",
    "            'duration': duration,\n",
    "            'sent': row['timestamp'],\n",
    "            'weight': duration + (30 if channel == 'sms' else 0)  # SMS weight = 30\n",
    "        }\n",
    "        \n",
    "        G.add_edge(src, dst, **edge_attrs)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1ddab",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def collapse_edges(G: nx.MultiDiGraph) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Collapse multilayer graph to weighted directed graph.\n",
    "    \n",
    "    Edge weight formula: duration_sec + 30 * sms_count\n",
    "    \n",
    "    Args:\n",
    "        G: MultiDiGraph with edge attributes\n",
    "    \n",
    "    Returns:\n",
    "        DiGraph with weighted edges\n",
    "    \"\"\"\n",
    "    G_collapsed = nx.DiGraph()\n",
    "    G_collapsed.add_nodes_from(G.nodes())\n",
    "    \n",
    "    # Aggregate edges between each pair\n",
    "    for u, v in G.edges():\n",
    "        if u == v:  # Skip self-loops\n",
    "            continue\n",
    "            \n",
    "        # Get all edges between u and v\n",
    "        edges_data = G.get_edge_data(u, v)\n",
    "        \n",
    "        total_weight = 0\n",
    "        total_duration = 0\n",
    "        sms_count = 0\n",
    "        call_count = 0\n",
    "        \n",
    "        for edge_key, edge_attrs in edges_data.items():\n",
    "            layer = edge_attrs['layer']\n",
    "            duration = edge_attrs.get('duration', 0)\n",
    "            \n",
    "            if layer == 'sms':\n",
    "                sms_count += 1\n",
    "            else:  # call\n",
    "                call_count += 1\n",
    "                total_duration += duration\n",
    "        \n",
    "        # Calculate weight: duration_sec + 30 * sms_count\n",
    "        weight = total_duration + 30 * sms_count\n",
    "        \n",
    "        if weight > 0:\n",
    "            G_collapsed.add_edge(u, v, weight=weight, \n",
    "                                duration=total_duration, \n",
    "                                sms_count=sms_count,\n",
    "                                call_count=call_count)\n",
    "    \n",
    "    return G_collapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e2761",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Build the multilayer graph\n",
    "print(\"Building multilayer graph...\")\n",
    "G_multilayer = build_multilayer_graph(df)\n",
    "print(f\"Multilayer graph: {G_multilayer.number_of_nodes()} nodes, {G_multilayer.number_of_edges()} edges\")\n",
    "\n",
    "# Collapse to weighted graph\n",
    "print(\"Collapsing to weighted graph...\")\n",
    "G = collapse_edges(G_multilayer)\n",
    "print(f\"Weighted graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34e802",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Core metric functions (≤ 25 lines each)\n",
    "\n",
    "def top_ties(G: nx.DiGraph, user: int, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Get user's top-k ties by weight.\"\"\"\n",
    "    if user not in G:\n",
    "        return []\n",
    "    \n",
    "    # Get outgoing edges with weights\n",
    "    edges = [(v, G[u][v]['weight']) for u, v in G.out_edges(user)]\n",
    "    edges.extend([(u, G[u][v]['weight']) for u, v in G.in_edges(user)])\n",
    "    \n",
    "    # Aggregate weights per neighbor\n",
    "    neighbor_weights = {}\n",
    "    for neighbor, weight in edges:\n",
    "        neighbor_weights[neighbor] = neighbor_weights.get(neighbor, 0) + weight\n",
    "    \n",
    "    # Sort by weight and return top-k\n",
    "    sorted_ties = sorted(neighbor_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_ties[:top_k]\n",
    "\n",
    "def channel_preference(G: nx.DiGraph, user1: int, user2: int) -> str:\n",
    "    \"\"\"Determine preferred channel (sms vs call) for a user pair.\"\"\"\n",
    "    if not G.has_edge(user1, user2) and not G.has_edge(user2, user1):\n",
    "        return \"no_interaction\"\n",
    "    \n",
    "    # Get edge data\n",
    "    edge_data = G.get_edge_data(user1, user2) or G.get_edge_data(user2, user1) or {}\n",
    "    \n",
    "    sms_count = edge_data.get('sms_count', 0)\n",
    "    call_count = edge_data.get('call_count', 0)\n",
    "    \n",
    "    if sms_count > call_count:\n",
    "        return \"sms\"\n",
    "    elif call_count > sms_count:\n",
    "        return \"call\"\n",
    "    else:\n",
    "        return \"equal\"\n",
    "\n",
    "def reciprocity(G: nx.DiGraph, user: int) -> float:\n",
    "    \"\"\"Calculate user's reciprocity score (bidirectional connections).\"\"\"\n",
    "    if user not in G:\n",
    "        return 0.0\n",
    "    \n",
    "    outgoing = set(G.successors(user))\n",
    "    incoming = set(G.predecessors(user))\n",
    "    \n",
    "    if not outgoing and not incoming:\n",
    "        return 0.0\n",
    "    \n",
    "    # Count bidirectional connections\n",
    "    bidirectional = outgoing & incoming\n",
    "    total_connections = outgoing | incoming\n",
    "    \n",
    "    return len(bidirectional) / len(total_connections) if total_connections else 0.0\n",
    "\n",
    "def detect_circles(G: nx.DiGraph, user: int) -> List[List[int]]:\n",
    "    \"\"\"Discover social circles for a user using community detection.\"\"\"\n",
    "    if user not in G:\n",
    "        return []\n",
    "    \n",
    "    # Create undirected subgraph around user (2-hop neighborhood)\n",
    "    neighbors = set(G.predecessors(user)) | set(G.successors(user))\n",
    "    two_hop = set()\n",
    "    for neighbor in neighbors:\n",
    "        two_hop.update(G.predecessors(neighbor))\n",
    "        two_hop.update(G.successors(neighbor))\n",
    "    \n",
    "    # Include user and immediate neighbors\n",
    "    subgraph_nodes = {user} | neighbors | two_hop\n",
    "    subgraph = G.subgraph(subgraph_nodes).to_undirected()\n",
    "    \n",
    "    # Use Louvain community detection\n",
    "    try:\n",
    "        communities = nx.community.louvain_communities(subgraph)\n",
    "        return [list(comm) for comm in communities if len(comm) > 1]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def relationship_trend(G: nx.DiGraph, user1: int, user2: int, df: pd.DataFrame) -> str:\n",
    "    \"\"\"Determine if relationship is growing or fading based on temporal data.\"\"\"\n",
    "    if not G.has_edge(user1, user2) and not G.has_edge(user2, user1):\n",
    "        return \"no_relationship\"\n",
    "    \n",
    "    # Get interactions between users\n",
    "    interactions = df[(df['src'] == user1) & (df['dst'] == user2) | \n",
    "                     (df['src'] == user2) & (df['dst'] == user1)]\n",
    "    \n",
    "    if len(interactions) < 2:\n",
    "        return \"insufficient_data\"\n",
    "    \n",
    "    # Split into first and second half\n",
    "    sorted_interactions = interactions.sort_values('timestamp')\n",
    "    mid_point = len(sorted_interactions) // 2\n",
    "    \n",
    "    first_half = sorted_interactions.iloc[:mid_point]\n",
    "    second_half = sorted_interactions.iloc[mid_point:]\n",
    "    \n",
    "    # Compare activity levels\n",
    "    first_activity = len(first_half)\n",
    "    second_activity = len(second_half)\n",
    "    \n",
    "    if second_activity > first_activity * 1.2:\n",
    "        return \"growing\"\n",
    "    elif first_activity > second_activity * 1.2:\n",
    "        return \"fading\"\n",
    "    else:\n",
    "        return \"stable\"\n",
    "\n",
    "def avg_reply_delay(G: nx.DiGraph, user1: int, user2: int, df: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculate average reply delay between two users.\"\"\"\n",
    "    # Get all interactions between users\n",
    "    interactions = df[(df['src'] == user1) & (df['dst'] == user2) | \n",
    "                     (df['src'] == user2) & (df['dst'] == user1)].sort_values('timestamp')\n",
    "    \n",
    "    if len(interactions) < 2:\n",
    "        return float('inf')\n",
    "    \n",
    "    delays = []\n",
    "    for i in range(len(interactions) - 1):\n",
    "        current = interactions.iloc[i]\n",
    "        next_interaction = interactions.iloc[i + 1]\n",
    "        \n",
    "        # If next interaction is from different user, it's a reply\n",
    "        if current['src'] != next_interaction['src']:\n",
    "            delay = next_interaction['timestamp'] - current['timestamp']  # Integer difference\n",
    "            delays.append(delay)\n",
    "    \n",
    "    return np.mean(delays) if delays else float('inf')\n",
    "\n",
    "def extrovert_score(G: nx.DiGraph, user: int) -> float:\n",
    "    \"\"\"Calculate extrovert score based on outgoing connections and bridge role.\"\"\"\n",
    "    if user not in G:\n",
    "        return 0.0\n",
    "    \n",
    "    # Outgoing degree (initiative)\n",
    "    out_degree = G.out_degree(user)\n",
    "    \n",
    "    # Bridge score (connectivity between communities)\n",
    "    neighbors = list(G.successors(user)) + list(G.predecessors(user))\n",
    "    if len(neighbors) < 2:\n",
    "        return out_degree\n",
    "    \n",
    "    # Calculate local clustering coefficient\n",
    "    local_clustering = nx.clustering(G, user) if len(neighbors) > 1 else 0\n",
    "    \n",
    "    # Extrovert score: high outgoing degree, low clustering (bridge role)\n",
    "    extrovert_score = out_degree * (1 - local_clustering)\n",
    "    \n",
    "    return extrovert_score\n",
    "\n",
    "def churn_drop(G: nx.DiGraph, user: int, df: pd.DataFrame, weeks: int = 4) -> float:\n",
    "    \"\"\"Detect weekly inbound-drop churn for a user.\"\"\"\n",
    "    if user not in G:\n",
    "        return 0.0\n",
    "    \n",
    "    # Get all interactions involving user\n",
    "    user_interactions = df[(df['src'] == user) | (df['dst'] == user)].sort_values('timestamp')\n",
    "    \n",
    "    if len(user_interactions) < 10:\n",
    "        return 0.0\n",
    "    \n",
    "    # Split into weekly periods (timestamps are relative integers)\n",
    "    min_time = user_interactions['timestamp'].min()\n",
    "    max_time = user_interactions['timestamp'].max()\n",
    "    total_time = max_time - min_time\n",
    "    \n",
    "    if total_time < weeks * 7:  # Need at least 4 weeks of data\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate weekly inbound counts\n",
    "    weekly_inbound = []\n",
    "    week_duration = total_time // weeks\n",
    "    \n",
    "    for week in range(weeks):\n",
    "        week_start = min_time + (week * week_duration)\n",
    "        week_end = week_start + week_duration if week < weeks - 1 else max_time + 1\n",
    "        \n",
    "        week_interactions = user_interactions[\n",
    "            (user_interactions['timestamp'] >= week_start) & \n",
    "            (user_interactions['timestamp'] < week_end) &\n",
    "            (user_interactions['dst'] == user)\n",
    "        ]\n",
    "        \n",
    "        weekly_inbound.append(len(week_interactions))\n",
    "    \n",
    "    # Calculate drop rate\n",
    "    if weekly_inbound[0] == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    drop_rate = (weekly_inbound[0] - weekly_inbound[-1]) / weekly_inbound[0]\n",
    "    return max(0, drop_rate)\n",
    "\n",
    "def find_spam_nodes(G: nx.DiGraph, threshold: float = 0.8) -> List[int]:\n",
    "    \"\"\"Find potential spam/harassment nodes based on activity patterns.\"\"\"\n",
    "    spam_nodes = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        out_degree = G.out_degree(node)\n",
    "        in_degree = G.in_degree(node)\n",
    "        \n",
    "        if out_degree == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate spam indicators\n",
    "        reply_ratio = in_degree / out_degree if out_degree > 0 else 0\n",
    "        activity_ratio = out_degree / (out_degree + in_degree) if (out_degree + in_degree) > 0 else 0\n",
    "        \n",
    "        # High outgoing, low incoming, low replies = potential spam\n",
    "        if activity_ratio > threshold and reply_ratio < (1 - threshold):\n",
    "            spam_nodes.append(node)\n",
    "    \n",
    "    return spam_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c199c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the core metrics\n",
    "print(\"Testing core metrics...\")\n",
    "\n",
    "# Test top_ties\n",
    "test_user = list(G.nodes())[0]\n",
    "top_connections = top_ties(G, test_user)\n",
    "print(f\"Top ties for user {test_user}: {top_connections[:3]}\")\n",
    "\n",
    "# Test channel_preference\n",
    "if G.number_of_edges() > 0:\n",
    "    edge = list(G.edges())[0]\n",
    "    preference = channel_preference(G, edge[0], edge[1])\n",
    "    print(f\"Channel preference for {edge}: {preference}\")\n",
    "\n",
    "# Test reciprocity\n",
    "reciprocity_score = reciprocity(G, test_user)\n",
    "print(f\"Reciprocity for user {test_user}: {reciprocity_score:.3f}\")\n",
    "\n",
    "# Test new metrics\n",
    "print(\"\\nTesting advanced metrics...\")\n",
    "\n",
    "# Test detect_circles\n",
    "circles = detect_circles(G, test_user)\n",
    "print(f\"Social circles for user {test_user}: {len(circles)} circles found\")\n",
    "\n",
    "# Test extrovert_score\n",
    "extrovert = extrovert_score(G, test_user)\n",
    "print(f\"Extrovert score for user {test_user}: {extrovert:.2f}\")\n",
    "\n",
    "# Test spam detection\n",
    "spam_nodes = find_spam_nodes(G)\n",
    "print(f\"Potential spam nodes found: {len(spam_nodes)}\")\n",
    "\n",
    "# Test relationship trend (if edge exists)\n",
    "if G.number_of_edges() > 0:\n",
    "    edge = list(G.edges())[0]\n",
    "    trend = relationship_trend(G, edge[0], edge[1], df)\n",
    "    print(f\"Relationship trend for {edge}: {trend}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics for all users\n",
    "print(\"Calculating comprehensive metrics for all users...\")\n",
    "\n",
    "user_metrics = []\n",
    "for user in G.nodes():\n",
    "    top_5_ties = top_ties(G, user, 5)\n",
    "    reciprocity_score = reciprocity(G, user)\n",
    "    extrovert_score_val = extrovert_score(G, user)\n",
    "    circles = detect_circles(G, user)\n",
    "    churn_rate = churn_drop(G, user, df)\n",
    "    \n",
    "    # Get user's total activity\n",
    "    out_degree = G.out_degree(user)\n",
    "    in_degree = G.in_degree(user)\n",
    "    total_activity = out_degree + in_degree\n",
    "    \n",
    "    user_metrics.append({\n",
    "        'user_id': user,\n",
    "        'top_ties': top_5_ties,\n",
    "        'reciprocity': reciprocity_score,\n",
    "        'extrovert_score': extrovert_score_val,\n",
    "        'num_circles': len(circles),\n",
    "        'churn_rate': churn_rate,\n",
    "        'out_degree': out_degree,\n",
    "        'in_degree': in_degree,\n",
    "        'total_activity': total_activity\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac96981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "print(\"Exporting results...\")\n",
    "\n",
    "# Save graph\n",
    "with open('graph.pkl', 'wb') as f:\n",
    "    pickle.dump(G, f)\n",
    "print(\"Saved graph.pkl\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame(user_metrics)\n",
    "metrics_df.to_csv('node_metrics.csv', index=False)\n",
    "print(\"Saved node_metrics.csv\")\n",
    "\n",
    "print(f\"Day 2 complete! Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\") "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
